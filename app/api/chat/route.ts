import { OpenAIStream, StreamingTextResponse } from 'ai';
import OpenAI from 'openai';

// ğŸ§  åˆå§‹åŒ– OpenAI SDK
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// âœ… å‘Šè¯‰ Vercel è¿™æ˜¯ Edge å‡½æ•°
export const runtime = 'edge';

// âœ… ç®€å•å†…å­˜é™æµå™¨ï¼ˆæ¯ IP æ¯åˆ†é’Ÿæœ€å¤š 5 æ¬¡ï¼‰
const ipCache = new Map<string, { count: number; lastReset: number }>();
const LIMIT_PER_MINUTE = 5;

function checkRateLimit(ip: string): boolean {
  const now = Date.now();
  const minute = 60 * 1000;

  const record = ipCache.get(ip);
  if (!record || now - record.lastReset > minute) {
    ipCache.set(ip, { count: 1, lastReset: now });
    return true;
  }

  if (record.count >= LIMIT_PER_MINUTE) return false;

  record.count += 1;
  ipCache.set(ip, record);
  return true;
}

// âœ… ä¸»å‡½æ•°å…¥å£
export async function POST(req: Request): Promise<Response> {
  const ip = req.headers.get('x-forwarded-for') || 'unknown';

  if (!checkRateLimit(ip)) {
    return new Response('â›”ï¸ Rate limit exceeded. Try again later.', { status: 429 });
  }

  const body = await req.json();
  const { messages } = body;

  // âœ… fallback æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼‰
  const fallbackModels = ['gpt-4o', 'gpt-4', 'gpt-3.5-turbo'];

  for (const model of fallbackModels) {
    try {
      const response = await openai.chat.completions.create({
        model,
        messages,
        stream: true,
      });

      const stream = OpenAIStream(response);
      return new StreamingTextResponse(stream);
    } catch (err: any) {
      console.warn(`âš ï¸ Model ${model} failed:`, err.message || err);
      continue; // å°è¯•ä¸‹ä¸€ä¸ª fallback æ¨¡å‹
    }
  }

  return new Response('âŒ All fallback models failed.', { status: 500 });
}
